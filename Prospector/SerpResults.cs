using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Net;
using System.Runtime.Serialization.Formatters.Binary;

using Browshot;
using HtmlAgilityPack;

namespace Prospector
{
    /// <summary>
    /// A collection of search engine results generated by one or more keywords
    /// </summary>
    public class SerpResults
    {
        // A bad idea normally to have your keys sitting out in the open. However! For the moment I need to 
        // be able to distrobute a working demo. Rest assured I'll close the account once I find a position.
        private const string BrowshotAPIKey = "4E6vWgxOdFCAdJdxXOzDBKHBIRmOLhv";

        private HtmlWeb web;
        public List<Site> sites;

        /// <summary>
        /// Creates a new SerpResults object
        /// </summary>
        public SerpResults()
        {
            web = new HtmlWeb();
            sites = new List<Site>();
        }

        /// <summary>
        /// Search for a keyword, storing X number of pages worth of site results
        /// </summary>
        /// <param name="query">The query to be run</param>
        /// <param name="keyword">The keyword you're searching for</param>
        /// <param name="pages">Number of results pages to crawl</param>
        public void DoSearch(string query, string keyword, int pages)
        {
            string completeQuery;

            for (int currentPage = 0; currentPage < pages; currentPage++)
            {
                Google.DelayToAvoidBan();

                // add pagination info to the query URL. eg:
                //   &num=100 is 100 results per page. Limited to 10 here 
                //   &start=0 would start at the 1st item of the 1st page, 101 would be 1st on 2nd page
                completeQuery = query + "&num=10&start=" + 10 * currentPage;

                HtmlDocument result = web.Load(completeQuery);

                // find and store each search result in the response. They're all in the format <li class="g">
                foreach (HtmlNode node in result.DocumentNode.SelectNodes("//li[@class='g']"))
                {
                    // for when the page doesn't match the expected format ie: google catches you crawling
                    if (node == null)
                    {
                        ErrorLogger.Log(new GooglingTooFrequentlyException());
                        break;
                    }
                    
                    string url = node.SelectSingleNode("div/div/cite").InnerHtml.Replace("https://", "").Split('/').First();

                    Site newSite = new Site("http://" + url, keyword);

                    // add if the site hasn't been seen this crawl
                    if (!sites.Contains(newSite))
                    {
                        sites.Add(newSite);
                        newSite.IdentifyCMS();
                    }
                }
            }
        }

        /// <summary>
        /// Examines the search results of a given site for spammy pages
        /// </summary>
        /// <param name="query">The search to be run</param>
        /// <param name="keyword">The suspected dodgy keyword</param>
        /// <param name="url">The site to be checked</param>
        public void IsSpam(string query, string keyword, string url)
        {
            Google.DelayToAvoidBan();

            HtmlDocument result = web.Load(query + "&num=20");
            string firstBad = "";

            // again, looking through <li class="g">'s to find individual search results. We're using ?? here 
            // as  SelectNodes() returns null when nothing is found, instead of returning a sensible empty list.
            foreach (HtmlNode node in result.DocumentNode.SelectNodes("//li[@class='g']") ??
                     new HtmlNodeCollection(null)) 
            {
                if (node == null)
                {
                    continue;
                }

                // grab details from each search result. Labelled example: http://i.imgur.com/j7Inh8V.jpg
                HtmlNode titleSection = node.SelectSingleNode("h3/a");

                string title = titleSection.InnerHtml;
                string snippet = node.SelectSingleNode("div/span").InnerHtml;
                
                // yuck
                string href = WebUtility.UrlDecode(titleSection.GetAttributeValue("href", "")).Split('&').First().Replace("/url?q=", "");
                
                // spam results often repeat the keyword in the title and snippet. If a site has results that 
                // don't have multiples of the keyword then we aren't dealing with a compromised site.
                if (!title.ToLower().Contains(keyword.ToLower()) && !snippet.ToLower().Contains(keyword.ToLower()))
                {
                    return;
                }

                // save the first dodgy result you find. This can be reported to site owners as proof
                if (firstBad == "")
                {
                    firstBad = href;
                }
            }

            sites.Add(new Site("http://" + url, keyword, firstBad));
        }

        /// <summary>
        /// Send Browshot to take screenshots of our suspect sites
        /// </summary>
        /// <param name="useCache">
        /// Allows Browshot to serve cached images for sites that have already been crawled. Saves credits, 
        /// defaults to true.
        /// </param>
        public void GetScreenshots(bool useCache = true)
        {
            BrowshotClient browshot = new BrowshotClient(BrowshotAPIKey);
            Dictionary<string, object> result;

            // Why am I counting backwards? 👻 It is a mystery!
            for(int i = sites.Count - 1; i >= 0; i--)
            {
                string redirUrl = "";
                Site site = sites[i];

                // there is no url to crawl for this site. Has previously been caused by bugs external to 
                // the method.
                if (site.badURL == "")
                {
                    ErrorLogger.Log(new Exception(), site.url);
                    sites.RemoveAt(i);
                    continue;
                }

                // some sites won't serve content to you if you don't have a believable browser agent, or if
                // you haven't been referred by Google's search results. Here we pretend to be both.
                HttpWebRequest request = ImpersonateBrowser(site.badURL);

                // also, some hacked sites perform a 302 redirect from the victim site to a landing page. we 
                // follow the redirect to obtain the page that we actually want to get a look at.
                try
                {
                    using (HttpWebResponse response = (HttpWebResponse)request.GetResponse())
                    {
                        redirUrl = response.ResponseUri.OriginalString;
                    }
                }
                catch (Exception e)
                {
                    ErrorLogger.Log(e, site.url);
                    sites.RemoveAt(i);
                    continue;
                }

                // 
                Hashtable arguments = new Hashtable();

                // each instance takes images of different resolutions and from different devices. Sadly the 
                // full list is only available on the BrowShot managment console.
                arguments.Add("instance_id", 64);
                arguments.Add("referer", Google.Refer(site.badURL));

                // caching images, saving money
                if (!useCache)
                {
                    arguments.Add("cache", 0);
                }

                result = browshot.ScreenshotCreate(redirUrl, arguments);

                try
                {
                    site.browshotID = int.Parse(result["id"].ToString());
                }
                catch (Exception e)
                {
                    ErrorLogger.Log(new BrowshotCreditsException(), site.url);
                    sites.RemoveAt(i);
                }
            }
        }

        /// <summary>
        /// Write the collected screenshots to file. Call GetScreenshots() first
        /// </summary>
        public void WriteScreenshots()
        {
            BrowshotClient browshot = new BrowshotClient(BrowshotAPIKey);
            Dictionary<string, object> result;

            Directory.CreateDirectory("out");

            for (int i = 0; i < sites.Count; i++)
            {
                if (sites[i].browshotID == 0)
                {
                    ErrorLogger.Log(new BrowshotException(), sites[i].url);
                    continue;
                }

                result = browshot.ScreenshotInfo(sites[i].browshotID);
                string status = result["status"].ToString();

                while (status != "finished" && status != "error")
                {
                    result = browshot.ScreenshotInfo(sites[i].browshotID);
                    status = result["status"].ToString();
                }

                if (status == "error")
                {
                    ErrorLogger.Log(new BrowshotException(), sites[i].url);
                    continue;
                }

                string imgLocation = result["screenshot_url"].ToString();
                string fileName = sites[i].url.Replace("http://", "").Replace("https://", "");
                WebClient webClient = new WebClient();
                webClient.DownloadFile(imgLocation, "out/" + fileName + ".png");
            }
        }

        /// <summary>
        /// Construct a web request which will pass for a legit request from Firefox
        /// </summary>
        /// <param name="url"></param>
        /// <returns>
        /// A web request which can bypass some counter-detection measures employed by the spammers
        /// </returns>
        private HttpWebRequest ImpersonateBrowser(string url)
        {
            HttpWebRequest request = (HttpWebRequest)WebRequest.Create(url);
            request.AllowAutoRedirect = true;
            request.Referer = Google.Refer(url);
            request.UserAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:35.0) Gecko/20100101 Firefox/35.0";
            request.Accept = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8";
            return request;
        }

        /// <summary>
        /// Write the results out to a .csv file. At the moment, you have to manually check these in ms
        /// excel or similar. Some results may be false positives.
        /// </summary>
        public void ToCSV()
        {
            string csv = "Site URL, Spam Example URL" + Environment.NewLine;

            foreach (Site site in sites)
            {
                if (site.badURL == null)
                {
                    continue;
                }

                csv += site.url + ", " + site.badURL + Environment.NewLine;
                File.WriteAllText("out/out.csv", csv);
            }
        }
    }
}
